{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DeepLabV3 with dropout=0.2 and pretrained=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]:   0%|          | 2/703 [00:53<5:09:56, 26.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 507\u001b[0m\n\u001b[0;32m    504\u001b[0m reduce_on_plateau \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAVED_MODEL_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDeepLabV3.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL2_REGULARIZATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATIENCE\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# Plot the final learning curves\u001b[39;00m\n\u001b[0;32m    519\u001b[0m plot_learning_curves(history)\n",
      "Cell \u001b[1;32mIn[4], line 407\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, save_path, base_lr, l2_reg, early_stopping_patience)\u001b[0m\n\u001b[0;32m    404\u001b[0m images, masks, masks_one_hot \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), masks\u001b[38;5;241m.\u001b[39mto(DEVICE), masks_one_hot\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    406\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 407\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m loss \u001b[38;5;241m=\u001b[39m hybrid_loss(outputs, masks, masks_one_hot, class_weights)\n\u001b[0;32m    409\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 352\u001b[0m, in \u001b[0;36mDeepLabV3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;66;03m# get feature maps from resnet\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m     f1, f2, f3, f4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# f4 => [B,2048,H/8, W/8] with atrous\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;66;03m# pass f4 to ASPP\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maspp(f4)                 \u001b[38;5;66;03m# => [B,256,H/8,W/8]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 230\u001b[0m, in \u001b[0;36mResNet50Encoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    228\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mlayer1(x)      \u001b[38;5;66;03m# => e.g. [B,256,H/4,W/4]\u001b[39;00m\n\u001b[0;32m    229\u001b[0m f2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mlayer2(f1)     \u001b[38;5;66;03m# => [B,512,H/8,W/8]\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m f3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf2\u001b[49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# => [B,1024,H/8,W/8] if we removed stride in layer3\u001b[39;00m\n\u001b[0;32m    231\u001b[0m f4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mlayer4(f3)     \u001b[38;5;66;03m# => [B,2048,H/8,W/8] or H/4 if further no stride\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f1, f2, f3, f4\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 141\u001b[0m, in \u001b[0;36mBottleneckWithDropout.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    138\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m    139\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m--> 141\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Insert dropout\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# ResNet-based classes\n",
    "import torchvision.models as tvmodels\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# ------------------- CONFIG -------------------\n",
    "BASE_PATH = \"C:/Users/User/Desktop/ai4mars/msl\"\n",
    "TRAIN_SPLIT = os.path.join(BASE_PATH, 'train_split.json')\n",
    "VAL_SPLIT   = os.path.join(BASE_PATH, 'val_split.json')\n",
    "CLASS_WEIGHTS_PATH = os.path.join(BASE_PATH, 'class_weights.json')\n",
    "SAVED_MODEL_DIR    = \"C:/Users/User/Desktop/saved_model\"\n",
    "os.makedirs(SAVED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "NUM_CLASSES        = 4    # Soil, Bedrock, Sand, Big Rock\n",
    "BATCH_SIZE         = 16\n",
    "EPOCHS             = 15\n",
    "LEARNING_RATE      = 1e-4   # Cosine schedule base\n",
    "L2_REGULARIZATION  = 1e-5\n",
    "PATIENCE           = 5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1) DATA LOADING\n",
    "def load_splits(split_path):\n",
    "    with open(split_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load the splitted data\n",
    "train_split = load_splits(TRAIN_SPLIT)\n",
    "val_split   = load_splits(VAL_SPLIT)\n",
    "\n",
    "# Load class weights\n",
    "with open(CLASS_WEIGHTS_PATH, 'r') as f:\n",
    "    class_weights_json = json.load(f)\n",
    "class_weights = torch.tensor(class_weights_json[\"class_weights\"], dtype=torch.float32)\n",
    "\n",
    "class RealMarsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data[idx]['image']\n",
    "        mask_path  = self.data[idx]['mask']\n",
    "\n",
    "        # Read & preprocess image\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        if image is None:\n",
    "            image = np.zeros((256,256,3), dtype=np.float32)\n",
    "        else:\n",
    "            image = image.astype(np.float32)/255.\n",
    "            image = cv2.resize(image, (256,256))\n",
    "        image_tensor = torch.from_numpy(image.transpose(2,0,1)).float()\n",
    "\n",
    "        # Read & preprocess mask\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            mask = np.full((256,256), fill_value=-1, dtype=np.int64)\n",
    "        else:\n",
    "            mask = mask.astype(np.int64)\n",
    "            mask = cv2.resize(mask, (256,256), interpolation=cv2.INTER_NEAREST)\n",
    "        mask[mask==255] = -1\n",
    "\n",
    "        # One-hot encoding\n",
    "        mask_one_hot = torch.zeros((NUM_CLASSES, *mask.shape), dtype=torch.float32)\n",
    "        for c in range(NUM_CLASSES):\n",
    "            mask_one_hot[c] = torch.from_numpy((mask==c).astype(np.float32))\n",
    "\n",
    "        return image_tensor, torch.tensor(mask, dtype=torch.long), mask_one_hot\n",
    "\n",
    "train_dataset = RealMarsDataset(train_split)\n",
    "val_dataset   = RealMarsDataset(val_split)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) HYBRID LOSS\n",
    "def hybrid_loss(pred, target, target_one_hot, class_weights, ignore_index=-1):\n",
    "    import torch.nn as nn\n",
    "    # Weighted CE\n",
    "    valid_target = target.clone()\n",
    "    valid_target[valid_target==ignore_index] = 0\n",
    "    wce = nn.CrossEntropyLoss(weight=class_weights, ignore_index=ignore_index)(pred, valid_target)\n",
    "\n",
    "    pred_softmax = F.softmax(pred, dim=1)\n",
    "\n",
    "    # Dice\n",
    "    dice_num = 2.0*(pred_softmax*target_one_hot).sum(dim=(2,3))\n",
    "    dice_den = (pred_softmax+target_one_hot).sum(dim=(2,3)) + 1e-6\n",
    "    dice = 1.0 - (dice_num/dice_den).mean()\n",
    "\n",
    "    # Tversky\n",
    "    alpha, beta = 0.7, 0.3\n",
    "    tp = (pred_softmax*target_one_hot).sum(dim=(2,3))\n",
    "    fn = (target_one_hot*(1-pred_softmax)).sum(dim=(2,3))\n",
    "    fp = ((1-target_one_hot)*pred_softmax).sum(dim=(2,3))\n",
    "    tversky_idx = tp/(tp+alpha*fn+beta*fp+1e-6)\n",
    "    tversky_loss = 1.0 - tversky_idx.mean()\n",
    "\n",
    "    # Focal\n",
    "    focal = -(target_one_hot*((1-pred_softmax)**2)*torch.log(pred_softmax+1e-6)).mean()\n",
    "\n",
    "    return wce + dice + tversky_loss + focal\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3) ENCODER with DROPOUT BOTTLEKNECK\n",
    "class BottleneckWithDropout(Bottleneck):\n",
    "    \"\"\"\n",
    "    Insert Dropout2d(p=0.2) after second 3x3 conv.\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, planes, *args, **kwargs):\n",
    "        self.dropout_p = kwargs.pop('dropout', 0.2)\n",
    "        super().__init__(inplanes, planes, *args, **kwargs)\n",
    "        self.dropout = nn.Dropout2d(p=self.dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # Insert dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet50Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Use a ResNet-50 backbone with atrous/dilated conv for the deeper layers,\n",
    "    but we keep it simpler by referencing torchvision's ResNet, injecting\n",
    "    BottleneckWithDropout. We'll partially load pretrained weights as well.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # 1) Build a standard resnet50\n",
    "        resnet_official = tvmodels.resnet50(pretrained=pretrained)\n",
    "\n",
    "        # 2) Replace the Bottleneck with our version that has dropout\n",
    "        # We'll replicate layer definitions from torchvision with 'block=BottleneckWithDropout'.\n",
    "        from torchvision.models.resnet import ResNet\n",
    "        from copy import deepcopy\n",
    "\n",
    "        class ResNetDrop(ResNet):\n",
    "            def __init__(self):\n",
    "                super().__init__(\n",
    "                    block=BottleneckWithDropout,\n",
    "                    layers=[3,4,6,3],\n",
    "                    zero_init_residual=False\n",
    "                )\n",
    "                # remove final FC, avgpool\n",
    "                del self.fc\n",
    "                del self.avgpool\n",
    "\n",
    "        self.base = ResNetDrop()\n",
    "        # We attempt to partially load from the official resnet50\n",
    "        official_dict = resnet_official.state_dict()\n",
    "        base_dict = self.base.state_dict()\n",
    "\n",
    "        filtered = {}\n",
    "        for k,v in official_dict.items():\n",
    "            # if shape matches, we copy\n",
    "            if k in base_dict and v.shape == base_dict[k].shape:\n",
    "                filtered[k] = v\n",
    "        base_dict.update(filtered)\n",
    "        self.base.load_state_dict(base_dict)\n",
    "\n",
    "        # Now we convert some layers for atrous conv in layer3, layer4 to enlarge FOV\n",
    "        # e.g. layer3, layer4 => dilation=2,4 ...\n",
    "        # We'll pick a moderate approach for DeepLabv3.\n",
    "        # Typically: layer3 is dilated=2, layer4 is dilated=4 => no further downsample in layer3.\n",
    "        self._convert_to_dilated(self.base.layer3, dilation=2)\n",
    "        self._convert_to_dilated(self.base.layer4, dilation=4)\n",
    "\n",
    "    def _convert_to_dilated(self, layer, dilation):\n",
    "        \"\"\" Convert stride in the first block to 1, and set both conv2's dilation & padding. \"\"\"\n",
    "        # For each Bottleneck in the layer\n",
    "        for i, block in enumerate(layer):\n",
    "            if i == 0:  \n",
    "                # remove stride in conv2\n",
    "                if block.downsample is not None:\n",
    "                    # remove stride in the downsample\n",
    "                    block.downsample[0].stride = (1,1)\n",
    "                block.conv2.dilation = (dilation,dilation)\n",
    "                block.conv2.padding  = (dilation,dilation)\n",
    "                block.conv2.stride   = (1,1)\n",
    "            else:\n",
    "                block.conv2.dilation = (dilation,dilation)\n",
    "                block.conv2.padding  = (dilation,dilation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The base is a partial resnet (no final pool/FC). \n",
    "        # We'll manually replicate forward.\n",
    "        x = self.base.conv1(x)\n",
    "        x = self.base.bn1(x)\n",
    "        x = self.base.relu(x)\n",
    "        x = self.base.maxpool(x)\n",
    "\n",
    "        f1 = self.base.layer1(x)      # => e.g. [B,256,H/4,W/4]\n",
    "        f2 = self.base.layer2(f1)     # => [B,512,H/8,W/8]\n",
    "        f3 = self.base.layer3(f2)     # => [B,1024,H/8,W/8] if we removed stride in layer3\n",
    "        f4 = self.base.layer4(f3)     # => [B,2048,H/8,W/8] or H/4 if further no stride\n",
    "\n",
    "        return f1, f2, f3, f4\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4) ASPP MODULE\n",
    "class ASPP(nn.Module):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling to capture multi-scale context.\n",
    "    We'll use parallel dilated convs with different rates, plus a global pool path.\n",
    "    To keep it lightweight, we reduce #channels in each branch.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=2048, out_channels=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout_p = dropout\n",
    "        # Branch1: 1x1 conv\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout)\n",
    "        )\n",
    "        # Branch2: 3x3 atrous conv (rate=6)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                      padding=6, dilation=6, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout)\n",
    "        )\n",
    "        # Branch3: 3x3 atrous conv (rate=12)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                      padding=12, dilation=12, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout)\n",
    "        )\n",
    "        # Branch4: 3x3 atrous conv (rate=18)\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "                      padding=18, dilation=18, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout)\n",
    "        )\n",
    "        # Global avg pool branch\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout)\n",
    "        )\n",
    "\n",
    "        # final conv for after concatenation\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(out_channels*5, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x => [B,2048,H,W] typically H,W = ~1/8 input size w/dilation\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        feat1 = self.branch1(x)\n",
    "        feat2 = self.branch2(x)\n",
    "        feat3 = self.branch3(x)\n",
    "        feat4 = self.branch4(x)\n",
    "\n",
    "        # global pooling\n",
    "        gp = self.global_pool(x)                 # => [B,2048,1,1]\n",
    "        gp = self.global_conv(gp)                # => [B,256,1,1]\n",
    "        gp = F.interpolate(gp, size=(h,w), mode='bilinear', align_corners=False)\n",
    "\n",
    "        cat = torch.cat([feat1, feat2, feat3, feat4, gp], dim=1)\n",
    "        out = self.project(cat)\n",
    "        return out\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5) DeeplabV3 HEAD\n",
    "class DeepLabV3Head(nn.Module):\n",
    "    \"\"\"\n",
    "    After ASPP, we do a small decode step (conv, upsample if needed).\n",
    "    \"\"\"\n",
    "    def __init__(self, aspp_out=256, num_classes=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout2d(p=dropout)\n",
    "        self.conv1 = nn.Conv2d(aspp_out, aspp_out//2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(aspp_out//2)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(aspp_out//2, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6) Full DeepLabV3\n",
    "class DeepLabV3(nn.Module):\n",
    "    \"\"\"\n",
    "    Proposed DeepLabV3 with:\n",
    "      - ResNet50Encoder( with BottleneckWithDropout + atrous convs )\n",
    "      - ASPP with multiple atrous rates\n",
    "      - Lightweight decode head => produce final segmentation logits\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=4, pretrained=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        print(f\"Initializing DeepLabV3 with dropout={dropout} and pretrained={pretrained}\")\n",
    "        self.encoder = ResNet50Encoder(pretrained=pretrained, dropout=dropout)\n",
    "        self.aspp    = ASPP(in_channels=2048, out_channels=256, dropout=dropout)\n",
    "        self.head    = DeepLabV3Head(aspp_out=256, num_classes=num_classes, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get feature maps from resnet\n",
    "        f1, f2, f3, f4 = self.encoder(x)  # f4 => [B,2048,H/8, W/8] with atrous\n",
    "        # pass f4 to ASPP\n",
    "        x = self.aspp(f4)                 # => [B,256,H/8,W/8]\n",
    "        # decode\n",
    "        logits = self.head(x)             # => [B,num_classes,H/8,W/8]\n",
    "        # upsample final => 256Ã—256\n",
    "        logits = F.interpolate(logits, scale_factor=8, mode='bilinear', align_corners=False)\n",
    "        return logits\n",
    "\n",
    "# ---------------------------------------------\n",
    "#         TRAIN FUNCTION (Cosine LR)\n",
    "# ---------------------------------------------\n",
    "def train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs, save_path,\n",
    "    base_lr=1e-4,  # initial LR\n",
    "    l2_reg=1e-7,\n",
    "    early_stopping_patience=5\n",
    "):\n",
    "    import torch.optim as optim\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=base_lr, weight_decay=l2_reg)\n",
    "\n",
    "    # Setup Cosine Annealing LR for 'num_epochs' cycles\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'val_loss': [],\n",
    "        'accuracy': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "\n",
    "    def mean_iou(pred, target, num_classes):\n",
    "        pred_labels = torch.argmax(pred, dim=1)\n",
    "        ious = []\n",
    "        for cls in range(num_classes):\n",
    "            intersection = ((pred_labels == cls) & (target == cls)).sum().item()\n",
    "            union = ((pred_labels == cls) | (target == cls)).sum().item()\n",
    "            if union > 0:\n",
    "                ious.append(intersection / union)\n",
    "        return np.mean(ious) if ious else 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        for images, masks, masks_one_hot in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            images, masks, masks_one_hot = images.to(DEVICE), masks.to(DEVICE), masks_one_hot.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = hybrid_loss(outputs, masks, masks_one_hot, class_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            valid_indices = (masks != -1)\n",
    "            train_correct += (preds[valid_indices] == masks[valid_indices]).sum().item()\n",
    "            train_total   += valid_indices.sum().item()\n",
    "\n",
    "        # Metrics for training\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / train_total if train_total>0 else 0.0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks, masks_one_hot in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                images, masks, masks_one_hot = images.to(DEVICE), masks.to(DEVICE), masks_one_hot.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = hybrid_loss(outputs, masks, masks_one_hot, class_weights)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                valid_indices = (masks != -1)\n",
    "                val_correct += (preds[valid_indices] == masks[valid_indices]).sum().item()\n",
    "                val_total   += valid_indices.sum().item()\n",
    "\n",
    "        val_loss     /= len(val_loader)\n",
    "        val_accuracy = val_correct / val_total if val_total>0 else 0.0\n",
    "\n",
    "        # Cosine LR step after each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_accuracy)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered. No improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# ---------------- PLOT LEARNING CURVES --------------\n",
    "def plot_learning_curves(history):\n",
    "    \"\"\"Plot training and validation loss, accuracy using the stored `history` dict.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label=\"Train Loss\", marker='o')\n",
    "    plt.plot(history['val_loss'], label=\"Validation Loss\", marker='o')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['accuracy'], label=\"Train Accuracy\", marker='o')\n",
    "    plt.plot(history['val_accuracy'], label=\"Validation Accuracy\", marker='o')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------- MAIN SCRIPT ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = DeepLabV3(num_classes=NUM_CLASSES)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_REGULARIZATION)\n",
    "\n",
    "    # Setup ReduceLROnPlateau\n",
    "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "    reduce_on_plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-10, verbose=1)\n",
    "\n",
    "    # Train\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=EPOCHS,\n",
    "        save_path=os.path.join(SAVED_MODEL_DIR, 'DeepLabV3.pth'),\n",
    "        base_lr=LEARNING_RATE,\n",
    "        l2_reg=L2_REGULARIZATION,\n",
    "        early_stopping_patience=PATIENCE\n",
    "    )\n",
    "\n",
    "    # Plot the final learning curves\n",
    "    plot_learning_curves(history)\n",
    "\n",
    "    print(\"Training complete. Results have been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load your test split and define a DataLoader similar to how you do it for train/val\n",
    "#    (Assuming you have test_split.json or something similar).\n",
    "test_split_path = os.path.join(BASE_PATH, \"test_split.json\")\n",
    "\n",
    "# Example RealMarsDataset usage for the test set (reusing the same class from your code)\n",
    "test_split_data = load_splits(test_split_path)\n",
    "test_dataset = RealMarsDataset(test_split_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# 2. Load the trained model and set it to eval mode\n",
    "model = DeepLabV3(num_classes=NUM_CLASSES)\n",
    "model_path = os.path.join(SAVED_MODEL_DIR, 'ResNet50_3.pth')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 3. Inference on the test set and gather predictions and targets\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks, _ in test_loader:  # We don't necessarily need masks_one_hot for evaluation\n",
    "        images = images.to(DEVICE)\n",
    "        masks  = masks.to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)  # shape: (batch_size, H, W)\n",
    "\n",
    "        # Move to CPU\n",
    "        preds_np  = preds.cpu().numpy()\n",
    "        masks_np  = masks.cpu().numpy()\n",
    "\n",
    "        # Flatten them, but exclude the -1 ignore pixels\n",
    "        for i in range(preds_np.shape[0]):\n",
    "            valid_indices = (masks_np[i] != -1)  # mask is -1 => ignore\n",
    "            valid_preds   = preds_np[i][valid_indices]\n",
    "            valid_targets = masks_np[i][valid_indices]\n",
    "            all_preds.extend(valid_preds.tolist())\n",
    "            all_targets.extend(valid_targets.tolist())\n",
    "\n",
    "# 4. Classification report (multi-class)\n",
    "#    We specify the class indices and their names for interpretability:\n",
    "class_names = [\"Soil\", \"Bedrock\", \"Sand\", \"Big Rock\"]\n",
    "report = classification_report(\n",
    "    all_targets,\n",
    "    all_preds,\n",
    "    labels=[0, 1, 2, 3],\n",
    "    target_names=class_names,\n",
    "    digits=4\n",
    ")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# 5. Confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds, labels=[0, 1, 2, 3])\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Compute class-wise and mean IoU\n",
    "#    We can adapt the mean_iou approach to aggregated data here\n",
    "def compute_iou(preds, targets, num_classes):\n",
    "    \"\"\"\n",
    "    preds: list or 1D array of predicted class indices\n",
    "    targets: list or 1D array of ground truth class indices\n",
    "    Returns: array of IoU values for each class, plus mean IoU\n",
    "    \"\"\"\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        intersection = 0\n",
    "        union = 0\n",
    "        for p, t in zip(preds, targets):\n",
    "            intersection += int(p == cls and t == cls)\n",
    "            union        += int((p == cls) or (t == cls))\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    return ious\n",
    "\n",
    "ious = compute_iou(all_preds, all_targets, NUM_CLASSES)\n",
    "print(\"Per-Class IoU:\")\n",
    "for idx, iou_val in enumerate(ious):\n",
    "    print(f\"  {class_names[idx]}: {iou_val:.4f}\")\n",
    "mean_iou = np.nanmean(ious)\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation map prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "BASE_PATH       = \"C:/Users/User/Desktop/ai4mars/msl\"\n",
    "TEST_SPLIT      = os.path.join(BASE_PATH, \"test_split.json\")\n",
    "SAVED_MODEL_DIR = \"C:/Users/User/Desktop/saved_model\"\n",
    "MODEL_PATH      = os.path.join(SAVED_MODEL_DIR, \"ResNet50_3.pth\")\n",
    "\n",
    "NUM_CLASSES     = 4\n",
    "DEVICE          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class_names = [\"Soil\", \"Bedrock\", \"Sand\", \"Big Rock\"]\n",
    "\n",
    "# Consistent BGR color map for display\n",
    "color_map = {\n",
    "    0: (0, 0, 128),    # Soil\n",
    "    1: (0, 128, 0),    # Bedrock\n",
    "    2: (0, 128, 128),  # Sand\n",
    "    3: (128, 0, 128),  # Big Rock\n",
    "}\n",
    "\n",
    "def load_test_split(test_split_path):\n",
    "    with open(test_split_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_preprocess_single_image(img_path):\n",
    "    \"\"\"Load an image, resize to 256x256, normalize [0..1], and convert to Tensor(C,H,W).\"\"\"\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    if image is None:\n",
    "        image = np.zeros((256, 256, 3), dtype=np.float32)\n",
    "    else:\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "    image_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
    "    return image_tensor\n",
    "\n",
    "def load_ground_truth_mask(mask_path):\n",
    "    \"\"\"Load ground-truth mask, resize, and return as (H,W) integer array.\"\"\"\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if mask is None:\n",
    "        mask = np.full((256, 256), fill_value=-1, dtype=np.int64)\n",
    "    else:\n",
    "        mask = mask.astype(np.int64)\n",
    "        mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "    return mask\n",
    "\n",
    "def colorize_mask(mask):\n",
    "    \"\"\"\n",
    "    Convert class indices in 'mask' to a BGR color image using 'color_map'.\n",
    "    mask shape: (H, W), integer class IDs.\n",
    "    Returns: (H,W,3) colored image.\n",
    "    \"\"\"\n",
    "    h, w = mask.shape\n",
    "    seg_vis = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for cls_idx, bgr in color_map.items():\n",
    "        seg_vis[mask == cls_idx] = bgr\n",
    "    return seg_vis\n",
    "\n",
    "def main():\n",
    "    # 1. Load test split\n",
    "    test_data = load_test_split(TEST_SPLIT)\n",
    "    if not test_data:\n",
    "        raise ValueError(\"Test split is empty or missing.\")\n",
    "\n",
    "    # 2. Select an image from the test split\n",
    "    image_id = 1239\n",
    "    if image_id < 0 or image_id >= len(test_data):\n",
    "        raise ValueError(f\"Invalid image_id: {image_id}, must be between 0 and {len(test_data)-1}.\")\n",
    "\n",
    "    sample = test_data[image_id]\n",
    "    image_path = sample[\"image\"]\n",
    "    mask_path  = sample[\"mask\"]  # Ground-truth label path\n",
    "\n",
    "    print(f\"Selected Image: {image_path}\")\n",
    "    print(f\"Ground Truth Label: {mask_path}\")\n",
    "\n",
    "    # 3. Load the trained ERFNet model\n",
    "    model = DeepLabV3(num_classes=NUM_CLASSES)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # 4. Preprocess the single image\n",
    "    image_tensor = load_preprocess_single_image(image_path).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Load original BGR image for display\n",
    "    original_bgr = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if original_bgr is None:\n",
    "        original_bgr = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "    else:\n",
    "        original_bgr = cv2.resize(original_bgr, (256, 256))\n",
    "\n",
    "    # Load ground truth mask\n",
    "    gt_mask = load_ground_truth_mask(mask_path)  # shape [256,256], class IDs\n",
    "    gt_color = colorize_mask(gt_mask)\n",
    "\n",
    "    # 5. Inference and measure time\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)  # shape [1, num_classes, 256,256]\n",
    "    end_time = time.time()\n",
    "    inference_time_ms = (end_time - start_time) * 1000\n",
    "\n",
    "    # 6. Convert to predicted mask\n",
    "    pred_mask = output.argmax(dim=1).squeeze(0).cpu().numpy()\n",
    "    pred_color = colorize_mask(pred_mask)\n",
    "\n",
    "    # 7. Visualization\n",
    "    # For clarity, we display 4 subplots: (1) original, (2) ground-truth, (3) predicted, (4) overlay\n",
    "    original_rgb = cv2.cvtColor(original_bgr, cv2.COLOR_BGR2RGB)\n",
    "    gt_rgb       = cv2.cvtColor(gt_color, cv2.COLOR_BGR2RGB)\n",
    "    pred_rgb     = cv2.cvtColor(pred_color, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Overlay: blend original and predicted\n",
    "    overlay_bgr = cv2.addWeighted(original_bgr, 0.5, pred_color, 0.5, 0)\n",
    "    overlay_rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Build legend patches\n",
    "    patches = []\n",
    "    for i, name in enumerate(class_names):\n",
    "        b, g, r = color_map[i]\n",
    "        color_rgb = (r/255, g/255, b/255)\n",
    "        patches.append(mpatches.Patch(color=color_rgb, label=name))\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Subplot 1: Original\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(original_rgb)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Subplot 2: Ground Truth\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(gt_rgb)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Subplot 3: Predicted\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(pred_rgb)\n",
    "    plt.title(\"Predicted\")\n",
    "    plt.axis('off')\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    # Subplot 4: Overlay\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(overlay_rgb)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f\"Inference Time: {inference_time_ms:.2f} ms\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
